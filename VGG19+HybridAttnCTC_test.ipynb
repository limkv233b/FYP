{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of VGG19+HybridAttnCTC.ipynb","provenance":[],"authorship_tag":"ABX9TyO4H66tEhrxfAXEyTO/75HJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXp08BcKkhfs","executionInfo":{"status":"ok","timestamp":1647525679267,"user_tz":-480,"elapsed":1107,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"9d93b5c4-e5d2-48f5-fa48-b31e091173c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'STR-Fewer-Labels'...\n","remote: Enumerating objects: 152, done.\u001b[K\n","remote: Counting objects: 100% (152/152), done.\u001b[K\n","remote: Compressing objects: 100% (126/126), done.\u001b[K\n","remote: Total 152 (delta 73), reused 52 (delta 18), pack-reused 0\u001b[K\n","Receiving objects: 100% (152/152), 1.63 MiB | 11.42 MiB/s, done.\n","Resolving deltas: 100% (73/73), done.\n","/content/STR-Fewer-Labels\n","HEAD is now at 9ca8c5e Update README.md\n"]}],"source":["!git clone https://github.com/ku21fan/STR-Fewer-Labels  # clone repo\n","%cd STR-Fewer-Labels\n","!git reset --hard"]},{"cell_type":"code","source":["!pip3 install lmdb pillow torchvision nltk natsort fire tensorboard tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1dQ_h2Zk4hr","executionInfo":{"status":"ok","timestamp":1647525683611,"user_tz":-480,"elapsed":4346,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"4dba972b-8a7c-4679-ad3e-1b7d2fb4c0f8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (0.99)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (5.5.0)\n","Collecting fire\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 30.3 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (2.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n","Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.37.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.35.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (1.44.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (0.6.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard) (3.17.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=7376b44b03e790f8582c6fafea34eea68e2573d55b6ec677deb56c53a3f5ab94\n","  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n","Successfully built fire\n","Installing collected packages: fire\n","Successfully installed fire-0.4.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!cp /content/gdrive/My\\ Drive/pretrained_model.zip /content/STR-Fewer-Labels\n","!cp /content/gdrive/My\\ Drive/data_CVPR2021.zip /content/STR-Fewer-Labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FS_UuNIhnW6W","executionInfo":{"status":"ok","timestamp":1647525884873,"user_tz":-480,"elapsed":201265,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"4a7bf604-42b6-47fd-c369-4b7306e29041"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!unzip pretrained_model.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kj4FnqtQdvCt","executionInfo":{"status":"ok","timestamp":1647525895520,"user_tz":-480,"elapsed":10662,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"ee2ef9f3-fd52-4ca3-d922-50273e7f72b5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  pretrained_model.zip\n","warning:  stripped absolute path spec from /\n","mapname:  conversion of  failed\n"," extracting: CRNN-PR.pth             \n"," extracting: TRBA-PL.pth             \n"," extracting: TRBA-PR.pth             \n"," extracting: CRNN-PL.pth             \n"," extracting: TRBA-Aug.pth            \n"," extracting: CRNN-Aug.pth            \n"," extracting: CRNN-Baseline-real.pth  \n"," extracting: TRBA-Baseline-real.pth  \n"," extracting: CRNN-Baseline-synth.pth  \n"," extracting: TRBA-Baseline-synth.pth  \n"]}]},{"cell_type":"code","source":["!unzip data_CVPR2021.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mL9yATMVk4j_","executionInfo":{"status":"ok","timestamp":1647526040490,"user_tz":-480,"elapsed":144978,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"65b04b76-4f64-498f-c53d-0333f26f2dfe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  data_CVPR2021.zip\n","   creating: data_CVPR2021/\n","   creating: data_CVPR2021/evaluation/\n","   creating: data_CVPR2021/evaluation/addition/\n","   creating: data_CVPR2021/evaluation/addition/5.COCO/\n","  inflating: data_CVPR2021/evaluation/addition/5.COCO/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/5.COCO/data.mdb  \n","   creating: data_CVPR2021/evaluation/addition/8.ArT/\n","  inflating: data_CVPR2021/evaluation/addition/8.ArT/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/8.ArT/data.mdb  \n","   creating: data_CVPR2021/evaluation/addition/9.LSVT/\n","  inflating: data_CVPR2021/evaluation/addition/9.LSVT/data.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/9.LSVT/lock.mdb  \n","   creating: data_CVPR2021/evaluation/addition/11.ReCTS/\n","  inflating: data_CVPR2021/evaluation/addition/11.ReCTS/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/11.ReCTS/data.mdb  \n","   creating: data_CVPR2021/evaluation/addition/7.Uber/\n","  inflating: data_CVPR2021/evaluation/addition/7.Uber/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/7.Uber/data.mdb  \n","   creating: data_CVPR2021/evaluation/addition/6.RCTW17/\n","  inflating: data_CVPR2021/evaluation/addition/6.RCTW17/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/6.RCTW17/data.mdb  \n","   creating: data_CVPR2021/evaluation/addition/10.MLT19/\n","  inflating: data_CVPR2021/evaluation/addition/10.MLT19/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/addition/10.MLT19/data.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/\n","   creating: data_CVPR2021/evaluation/benchmark/CUTE80/\n","  inflating: data_CVPR2021/evaluation/benchmark/CUTE80/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/CUTE80/data.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/IC13_1015/\n","  inflating: data_CVPR2021/evaluation/benchmark/IC13_1015/data.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/IC13_1015/lock.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/IC15_2077/\n","  inflating: data_CVPR2021/evaluation/benchmark/IC15_2077/lock.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/IC15_2077/data.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/IIIT5k_3000/\n","  inflating: data_CVPR2021/evaluation/benchmark/IIIT5k_3000/data.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/IIIT5k_3000/lock.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/SVT/\n","  inflating: data_CVPR2021/evaluation/benchmark/SVT/data.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/SVT/lock.mdb  \n","   creating: data_CVPR2021/evaluation/benchmark/SVTP/\n","  inflating: data_CVPR2021/evaluation/benchmark/SVTP/data.mdb  \n","  inflating: data_CVPR2021/evaluation/benchmark/SVTP/lock.mdb  \n","   creating: data_CVPR2021/training/\n","   creating: data_CVPR2021/training/unlabel/\n","   creating: data_CVPR2021/training/unlabel/U1.Book32/\n","  inflating: data_CVPR2021/training/unlabel/U1.Book32/lock.mdb  \n","  inflating: data_CVPR2021/training/unlabel/U1.Book32/data.mdb  \n","   creating: data_CVPR2021/training/unlabel/U2.TextVQA/\n","  inflating: data_CVPR2021/training/unlabel/U2.TextVQA/lock.mdb  \n","  inflating: data_CVPR2021/training/unlabel/U2.TextVQA/data.mdb  \n","   creating: data_CVPR2021/training/unlabel/U3.STVQA/\n","  inflating: data_CVPR2021/training/unlabel/U3.STVQA/lock.mdb  \n","  inflating: data_CVPR2021/training/unlabel/U3.STVQA/data.mdb  \n","   creating: data_CVPR2021/training/label/\n","   creating: data_CVPR2021/training/label/real/\n","   creating: data_CVPR2021/training/label/real/5.COCO/\n","  inflating: data_CVPR2021/training/label/real/5.COCO/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/5.COCO/data.mdb  \n","   creating: data_CVPR2021/training/label/real/8.ArT/\n","  inflating: data_CVPR2021/training/label/real/8.ArT/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/8.ArT/data.mdb  \n","   creating: data_CVPR2021/training/label/real/1.SVT/\n","  inflating: data_CVPR2021/training/label/real/1.SVT/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/1.SVT/data.mdb  \n","   creating: data_CVPR2021/training/label/real/9.LSVT/\n","  inflating: data_CVPR2021/training/label/real/9.LSVT/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/9.LSVT/data.mdb  \n","   creating: data_CVPR2021/training/label/real/2.IIIT/\n","  inflating: data_CVPR2021/training/label/real/2.IIIT/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/2.IIIT/data.mdb  \n","   creating: data_CVPR2021/training/label/real/3.IC13/\n","  inflating: data_CVPR2021/training/label/real/3.IC13/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/3.IC13/data.mdb  \n","   creating: data_CVPR2021/training/label/real/4.IC15/\n","  inflating: data_CVPR2021/training/label/real/4.IC15/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/4.IC15/data.mdb  \n","   creating: data_CVPR2021/training/label/real/7.Uber/\n","  inflating: data_CVPR2021/training/label/real/7.Uber/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/7.Uber/data.mdb  \n","   creating: data_CVPR2021/training/label/real/6.RCTW17/\n","  inflating: data_CVPR2021/training/label/real/6.RCTW17/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/6.RCTW17/data.mdb  \n","   creating: data_CVPR2021/training/label/real/11.ReCTS/\n","  inflating: data_CVPR2021/training/label/real/11.ReCTS/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/11.ReCTS/data.mdb  \n","   creating: data_CVPR2021/training/label/real/10.MLT19/\n","  inflating: data_CVPR2021/training/label/real/10.MLT19/lock.mdb  \n","  inflating: data_CVPR2021/training/label/real/10.MLT19/data.mdb  \n","   creating: data_CVPR2021/training/label/synth/\n","   creating: data_CVPR2021/validation/\n","   creating: data_CVPR2021/validation/11.ReCTS/\n","  inflating: data_CVPR2021/validation/11.ReCTS/lock.mdb  \n","  inflating: data_CVPR2021/validation/11.ReCTS/data.mdb  \n","   creating: data_CVPR2021/validation/1.SVT/\n","  inflating: data_CVPR2021/validation/1.SVT/lock.mdb  \n","  inflating: data_CVPR2021/validation/1.SVT/data.mdb  \n","   creating: data_CVPR2021/validation/2.IIIT/\n","  inflating: data_CVPR2021/validation/2.IIIT/lock.mdb  \n","  inflating: data_CVPR2021/validation/2.IIIT/data.mdb  \n","   creating: data_CVPR2021/validation/3.IC13/\n","  inflating: data_CVPR2021/validation/3.IC13/lock.mdb  \n","  inflating: data_CVPR2021/validation/3.IC13/data.mdb  \n","   creating: data_CVPR2021/validation/4.IC15/\n","  inflating: data_CVPR2021/validation/4.IC15/lock.mdb  \n","  inflating: data_CVPR2021/validation/4.IC15/data.mdb  \n","   creating: data_CVPR2021/validation/5.COCO/\n","  inflating: data_CVPR2021/validation/5.COCO/lock.mdb  \n","  inflating: data_CVPR2021/validation/5.COCO/data.mdb  \n","   creating: data_CVPR2021/validation/6.RCTW17/\n","  inflating: data_CVPR2021/validation/6.RCTW17/lock.mdb  \n","  inflating: data_CVPR2021/validation/6.RCTW17/data.mdb  \n","   creating: data_CVPR2021/validation/7.Uber/\n","  inflating: data_CVPR2021/validation/7.Uber/lock.mdb  \n","  inflating: data_CVPR2021/validation/7.Uber/data.mdb  \n","   creating: data_CVPR2021/validation/8.ArT/\n","  inflating: data_CVPR2021/validation/8.ArT/lock.mdb  \n","  inflating: data_CVPR2021/validation/8.ArT/data.mdb  \n","   creating: data_CVPR2021/validation/9.LSVT/\n","  inflating: data_CVPR2021/validation/9.LSVT/lock.mdb  \n","  inflating: data_CVPR2021/validation/9.LSVT/data.mdb  \n","   creating: data_CVPR2021/validation/10.MLT19/\n","  inflating: data_CVPR2021/validation/10.MLT19/lock.mdb  \n","  inflating: data_CVPR2021/validation/10.MLT19/data.mdb  \n"]}]},{"cell_type":"code","source":["!wget -O dict.txt https://raw.githubusercontent.com/dwyl/english-words/master/words.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CbOlkaSgu3H0","executionInfo":{"status":"ok","timestamp":1647526041154,"user_tz":-480,"elapsed":669,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"6d52288b-423b-48ad-a34b-f822deeef063"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-17 14:07:20--  https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4862992 (4.6M) [text/plain]\n","Saving to: ‘dict.txt’\n","\n","dict.txt            100%[===================>]   4.64M  --.-KB/s    in 0.03s   \n","\n","2022-03-17 14:07:20 (149 MB/s) - ‘dict.txt’ saved [4862992/4862992]\n","\n"]}]},{"cell_type":"code","source":["!pip install --quiet larq larq-zoo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQ6pqBWonr2T","executionInfo":{"status":"ok","timestamp":1647526046741,"user_tz":-480,"elapsed":5590,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"8e9b6272-24db-4733-827f-4b88f446a1d7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |█████                           | 10 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 30 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 40 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 51 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 61 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n","\u001b[K     |████████████████████████████████| 57 kB 6.0 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 2.1.3 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install textdistance"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PCtw3xVatlqd","executionInfo":{"status":"ok","timestamp":1647526049378,"user_tz":-480,"elapsed":2645,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"0cf1fc3a-9618-4a87-eb12-790a420201f7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textdistance\n","  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n","Installing collected packages: textdistance\n","Successfully installed textdistance-4.2.2\n"]}]},{"cell_type":"code","source":["#@title Edit Demo { form-width: \"20%\" }\n","%%writefile demo.py\n","import sys\n","import string\n","import argparse\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.utils.data\n","import torch.nn.functional as F\n","\n","from utils import CTCLabelConverter, AttnLabelConverter\n","from dataset import RawDataset, AlignCollate\n","from model import Model\n","import pandas as pd\n","import numpy as np\n","import textdistance\n","import re\n","from collections import Counter\n","from IPython.display import display\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def demo(opt):\n","    \"\"\"model configuration\"\"\"\n","    if \"CTC\" in opt.Prediction:\n","        converter = CTCLabelConverter(opt.character)\n","    else:\n","        converter = AttnLabelConverter(opt.character)\n","        opt.sos_token_index = converter.dict[\"[SOS]\"]\n","        opt.eos_token_index = converter.dict[\"[EOS]\"]\n","    opt.num_class = len(converter.character)\n","\n","    model = Model(opt)\n","    print(\n","        \"model input parameters\",\n","        opt.imgH,\n","        opt.imgW,\n","        opt.num_fiducial,\n","        opt.input_channel,\n","        opt.output_channel,\n","        opt.hidden_size,\n","        opt.num_class,\n","        opt.batch_max_length,\n","        opt.Transformation,\n","        opt.FeatureExtraction,\n","        opt.SequenceModeling,\n","        opt.Prediction,\n","    )\n","    model = torch.nn.DataParallel(model).to(device)\n","\n","    # load model\n","    print(\"loading pretrained model from %s\" % opt.saved_model)\n","    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n","\n","    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n","    AlignCollate_demo = AlignCollate(opt, mode=\"test\")\n","    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n","    demo_loader = torch.utils.data.DataLoader(\n","        demo_data,\n","        batch_size=opt.batch_size,\n","        shuffle=False,\n","        num_workers=int(opt.workers),\n","        collate_fn=AlignCollate_demo,\n","        pin_memory=True,\n","    )\n","\n","    # predict\n","    model.eval()\n","    with torch.no_grad():\n","        log = open(f\"./log_demo_result.txt\", \"w\")\n","        for image_tensors, image_path_list in demo_loader:\n","            batch_size = image_tensors.size(0)\n","            image = image_tensors.to(device)\n","\n","            if \"CTC\" in opt.Prediction:\n","                preds = model(image)\n","            else:\n","                # For max length prediction\n","                text_for_pred = (\n","                    torch.LongTensor(batch_size)\n","                    .fill_(converter.dict[\"[SOS]\"])\n","                    .to(device)\n","                )\n","                preds = model(image, text_for_pred, is_train=False)\n","\n","            # Select max probabilty (greedy decoding) then decode index to character\n","            preds_size = torch.IntTensor([preds.size(1)] * batch_size).to(device)\n","            _, preds_index = preds.max(2)\n","            preds_str = converter.decode(preds_index, preds_size)\n","\n","            preds_prob = F.softmax(preds, dim=2)\n","            preds_max_prob, _ = preds_prob.max(dim=2)\n","            output = []\n","            for img_name, pred, pred_max_prob in zip(\n","                image_path_list, preds_str, preds_max_prob\n","            ):\n","                if \"Attn\" in opt.Prediction:\n","                    pred_EOS = pred.find(\"[EOS]\")\n","                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n","                    pred_max_prob = pred_max_prob[:pred_EOS]\n","\n","                # calculate confidence score (= multiply of pred_max_prob)\n","                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n","\n","                # print(f\"{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\")\n","                output.append([img_name,pred,float(confidence_score)])\n","                log.write(f\"{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n\")\n","\n","        log.close()\n","        return output\n","def checkspelling(input_word_A, input_word_CTC, V): \n","    V = list(V)\n","    word_freq = {}  \n","    word_freq = Counter(V)\n","    probs = {}     \n","    Total = sum(word_freq.values())    \n","    for k in word_freq.keys():\n","        probs[k] = word_freq[k]/Total\n","    input_word_A = input_word_A.lower()\n","    input_word_CTC = input_word_CTC.lower()\n","\n","    simil_A = 0.0\n","    if input_word_A in V:\n","        simil_A = 1.0\n","    else:\n","        sim = [1-(textdistance.Jaccard(qval=2).distance(v,input_word_A)) for v in word_freq.keys()]\n","        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n","        df = df.rename(columns={'index':'Word', 0:'Prob'})\n","        df['Similarity'] = sim\n","        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\n","        simil_A = float(output.iloc[0,2])\n","    \n","    \n","    simil_CTC = 0.0\n","    if input_word_CTC in V:\n","        simil_CTC = 1.0\n","    else:\n","        sim = [1-(textdistance.Jaccard(qval=2).distance(v,input_word_CTC)) for v in word_freq.keys()]\n","        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n","        df = df.rename(columns={'index':'Word', 0:'Prob'})\n","        df['Similarity'] = sim\n","        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\n","        simil_CTC = float(output.iloc[0,2])\n","    if simil_CTC>simil_A:\n","      return int(1)\n","    else:\n","      return int(2)\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\n","        \"--image_folder\",\n","        default=\"demo_image/\",\n","        help=\"path to image_folder which contains text images\",\n","    )\n","    parser.add_argument(\n","        \"--workers\", type=int, help=\"number of data loading workers\", default=4\n","    )\n","    parser.add_argument(\"--batch_size\", type=int, default=192, help=\"input batch size\")\n","    parser.add_argument(\n","        \"--saved_model\", required=True, help=\"path to saved_model to evaluation\"\n","    )\n","    \"\"\" Data processing \"\"\"\n","    parser.add_argument(\n","        \"--batch_max_length\", type=int, default=25, help=\"maximum-label-length\"\n","    )\n","    parser.add_argument(\n","        \"--imgH\", type=int, default=32, help=\"the height of the input image\"\n","    )\n","    parser.add_argument(\n","        \"--imgW\", type=int, default=100, help=\"the width of the input image\"\n","    )\n","    parser.add_argument(\n","        \"--character\",\n","        type=str,\n","        default=\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\",\n","        help=\"character label\",\n","    )\n","    parser.add_argument(\"--Aug\", type=str, default=\"None\", help=\"placeholder\")\n","    parser.add_argument(\"--semi\", type=str, default=\"None\", help=\"placeholder\")\n","    \"\"\" Model Architecture \"\"\"\n","    parser.add_argument(\"--model_name\", type=str, required=True, help=\"CRNN|TRBA\")\n","    parser.add_argument(\n","        \"--num_fiducial\",\n","        type=int,\n","        default=20,\n","        help=\"number of fiducial points of TPS-STN\",\n","    )\n","    parser.add_argument(\n","        \"--input_channel\",\n","        type=int,\n","        default=3,\n","        help=\"the number of input channel of Feature extractor\",\n","    )\n","    parser.add_argument(\n","        \"--output_channel\",\n","        type=int,\n","        default=512,\n","        help=\"the number of output channel of Feature extractor\",\n","    )\n","    parser.add_argument(\n","        \"--hidden_size\", type=int, default=256, help=\"the size of the LSTM hidden state\"\n","    )\n","\n","    opt = parser.parse_args()\n","\n","    if opt.model_name == \"CRNN\":\n","        opt.Transformation = \"None\"\n","        opt.FeatureExtraction = \"VGG\"\n","        opt.SequenceModeling = \"BiLSTM\"\n","        opt.Prediction = \"CTC\"\n","\n","    elif opt.model_name == \"TRBA\":\n","        opt.Transformation = \"TPS\"\n","        opt.FeatureExtraction = \"ResNet\"\n","        opt.SequenceModeling = \"BiLSTM\"\n","        opt.Prediction = \"Attn\"\n","\n","    cudnn.benchmark = True  # It fasten training.\n","    cudnn.deterministic = True\n","\n","    opt.num_gpu = torch.cuda.device_count()\n","    if opt.num_gpu > 1:\n","        print(\n","            \"We recommend to use 1 GPU, check your GPU number, you would miss CUDA_VISIBLE_DEVICES=0 or typo\"\n","        )\n","        print(\"To use multi-gpu setting, remove or comment out these lines\")\n","        sys.exit()\n","\n","    if sys.platform == \"win32\":\n","        opt.workers = 0\n","\n","    output = demo(opt)\n","    opt.Transformation = \"None\"\n","    opt.FeatureExtraction = \"VGG\"\n","    opt.SequenceModeling = \"BiLSTM\"\n","    opt.Prediction = \"CTC\"\n","    opt.saved_model = \"/content/STR-Fewer-Labels/CRNN-PR.pth\"\n","    output2 = demo(opt)\n","    \n","    with open('dict.txt', 'r') as f:\n","        file_name_data = f.read()\n","        file_name_data=file_name_data.lower()\n","        V = set(file_name_data.split(\"\\n\")) \n","    dashed_line = \"-\" * 80\n","    head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\t{\"confidence score\"}'\n","    print(f\"{dashed_line}\\n{head}\\n{dashed_line}\")\n","    for k in range(len(output)):\n","      if (checkspelling(output[k][1], output2[k][1], V)) == 1:\n","        print(f\"{output2[k][0]:25s}\\t{output2[k][1]:25s}\\t{output2[k][2]:0.4f}\")\n","      else:\n","        print(f\"{output[k][0]:25s}\\t{output[k][1]:25s}\\t{output[k][2]:0.4f}\")\n","        "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"OJN6HxqboJDA","executionInfo":{"status":"ok","timestamp":1647526049378,"user_tz":-480,"elapsed":18,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"65a80bd5-df5a-43c7-9baf-17cc622c3ae6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting demo.py\n"]}]},{"cell_type":"code","source":["#@title Edit Test { form-width: \"20%\" }\n","%%writefile test.py\n","import os\n","import sys\n","import time\n","import string\n","import argparse\n","import re\n","from datetime import date\n","\n","import torch\n","import torch.backends.cudnn as cudnn\n","import torch.utils.data\n","import torch.nn.functional as F\n","import numpy as np\n","from nltk.metrics.distance import edit_distance\n","from tqdm import tqdm\n","\n","from utils import CTCLabelConverter, AttnLabelConverter, Averager\n","from dataset import hierarchical_dataset, AlignCollate\n","from model import Model\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def benchmark_all_eval(model, criterion, converter, opt, calculate_infer_time=False):\n","\n","    if opt.eval_type == \"benchmark\":\n","        \"\"\"evaluation with 6 benchmark evaluation datasets\"\"\"\n","        eval_data_list = [\n","            \"IIIT5k_3000\",\n","            \"SVT\",\n","            \"IC13_1015\",\n","            \"IC15_2077\",\n","            \"SVTP\",\n","            \"CUTE80\",\n","        ]\n","        opt.eval_data = \"data_CVPR2021/evaluation/benchmark/\"\n","\n","    elif opt.eval_type == \"addition\":\n","        \"\"\"evaluation with 7 additionally collected evaluation datasets\"\"\"\n","        eval_data_list = [\n","            \"5.COCO\",\n","            \"6.RCTW17\",\n","            \"7.Uber\",\n","            \"8.ArT\",\n","            \"9.LSVT\",\n","            \"10.MLT19\",\n","            \"11.ReCTS\",\n","        ]\n","        opt.eval_data = \"data_CVPR2021/evaluation/addition/\"\n","\n","    if calculate_infer_time:\n","        eval_batch_size = (\n","            1  # batch_size should be 1 to calculate the GPU inference time per image.\n","        )\n","    else:\n","        eval_batch_size = opt.batch_size\n","\n","    accuracy_list = []\n","    total_forward_time = 0\n","    total_eval_data_number = 0\n","    total_correct_number = 0\n","    log = open(f\"./result/{opt.exp_name}/log_all_evaluation.txt\", \"a\")\n","    dashed_line = \"-\" * 80\n","    print(dashed_line)\n","    log.write(dashed_line + \"\\n\")\n","    for eval_data in eval_data_list:\n","        eval_data_path = os.path.join(opt.eval_data, eval_data)\n","        AlignCollate_eval = AlignCollate(opt, mode=\"test\")\n","        eval_data, eval_data_log = hierarchical_dataset(\n","            root=eval_data_path, opt=opt, mode=\"test\"\n","        )\n","        eval_loader = torch.utils.data.DataLoader(\n","            eval_data,\n","            batch_size=eval_batch_size,\n","            shuffle=False,\n","            num_workers=int(opt.workers),\n","            collate_fn=AlignCollate_eval,\n","            pin_memory=True,\n","        )\n","\n","        _, accuracy_by_best_model, _, _, _, infer_time, length_of_data = validation(\n","            model, criterion, eval_loader, converter, opt, tqdm_position=0\n","        )\n","        accuracy_list.append(f\"{accuracy_by_best_model:0.2f}\")\n","        total_forward_time += infer_time\n","        total_eval_data_number += len(eval_data)\n","        total_correct_number += accuracy_by_best_model * length_of_data\n","        log.write(eval_data_log)\n","        print(f\"Acc {accuracy_by_best_model:0.2f}\")\n","        log.write(f\"Acc {accuracy_by_best_model:0.2f}\\n\")\n","        print(dashed_line)\n","        log.write(dashed_line + \"\\n\")\n","\n","    averaged_forward_time = total_forward_time / total_eval_data_number * 1000\n","    total_accuracy = total_correct_number / total_eval_data_number\n","    params_num = sum([np.prod(p.size()) for p in model.parameters()])\n","\n","    eval_log = \"accuracy: \"\n","    for name, accuracy in zip(eval_data_list, accuracy_list):\n","        eval_log += f\"{name}: {accuracy}\\t\"\n","    eval_log += f\"total_accuracy: {total_accuracy:0.2f}\\t\"\n","    eval_log += f\"averaged_infer_time: {averaged_forward_time:0.3f}\\t# parameters: {params_num/1e6:0.2f}\"\n","    print(eval_log)\n","    log.write(eval_log + \"\\n\")\n","\n","    # for convenience\n","    print(\"\\t\".join(accuracy_list))\n","    print(f\"Total_accuracy:{total_accuracy:0.2f}\")\n","    log.write(\"\\t\".join(accuracy_list) + \"\\n\")\n","    log.write(f\"Total_accuracy:{total_accuracy:0.2f}\" + \"\\n\")\n","    log.close()\n","\n","    # for convenience\n","    today = date.today()\n","    if opt.log_multiple_test:\n","        log_all_model = open(f\"./evaluation_log/log_multiple_test_{today}.txt\", \"a\")\n","        log_all_model.write(\"\\t\".join(accuracy_list) + \"\\n\")\n","    else:\n","        log_all_model = open(\n","            f\"./evaluation_log/log_all_model_evaluation_{today}.txt\", \"a\"\n","        )\n","        log_all_model.write(\n","            f\"./result/{opt.exp_name}\\tTotal_accuracy:{total_accuracy:0.2f}\\n\"\n","        )\n","        log_all_model.write(\"\\t\".join(accuracy_list) + \"\\n\")\n","    log_all_model.close()\n","\n","    return total_accuracy, eval_data_list, accuracy_list\n","\n","\n","def validation(model, criterion, eval_loader, converter, opt, tqdm_position=1):\n","    \"\"\"validation or evaluation\"\"\"\n","    n_correct = 0\n","    norm_ED = 0\n","    length_of_data = 0\n","    infer_time = 0\n","    valid_loss_avg = Averager()\n","\n","    for i, (image_tensors, labels) in tqdm(\n","        enumerate(eval_loader),\n","        total=len(eval_loader),\n","        position=tqdm_position,\n","        leave=False,\n","    ):\n","        batch_size = image_tensors.size(0)\n","        length_of_data = length_of_data + batch_size\n","        image = image_tensors.to(device)\n","        # For max length prediction\n","        labels_index, labels_length = converter.encode(\n","            labels, batch_max_length=opt.batch_max_length\n","        )\n","\n","        if \"CTC\" in opt.Prediction:\n","            start_time = time.time()\n","            preds = model(image)\n","            forward_time = time.time() - start_time\n","\n","            # Calculate evaluation loss for CTC deocder.\n","            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n","            # permute 'preds' to use CTCloss format\n","            cost = criterion(\n","                preds.log_softmax(2).permute(1, 0, 2),\n","                labels_index,\n","                preds_size,\n","                labels_length,\n","            )\n","\n","        else:\n","            text_for_pred = (\n","                torch.LongTensor(batch_size).fill_(converter.dict[\"[SOS]\"]).to(device)\n","            )\n","\n","            start_time = time.time()\n","            preds = model(image, text_for_pred, is_train=False)\n","            forward_time = time.time() - start_time\n","\n","            target = labels_index[:, 1:]  # without [SOS] Symbol\n","            cost = criterion(\n","                preds.contiguous().view(-1, preds.shape[-1]),\n","                target.contiguous().view(-1),\n","            )\n","\n","        # select max probabilty (greedy decoding) then decode index to character\n","        _, preds_index = preds.max(2)\n","        preds_size = torch.IntTensor([preds.size(1)] * preds_index.size(0)).to(device)\n","        preds_str = converter.decode(preds_index, preds_size)\n","\n","        infer_time += forward_time\n","        valid_loss_avg.add(cost)\n","\n","        # calculate accuracy & confidence score\n","        preds_prob = F.softmax(preds, dim=2)\n","        preds_max_prob, _ = preds_prob.max(dim=2)\n","        confidence_score_list = []\n","        len_all = list(zip(labels, preds_str, preds_max_prob))\n","        stop = int(len(len_all)*0.9)\n","        for index in zip(labels, preds_str, preds_max_prob):\n","            if stop == 0:\n","              break\n","            stop-=1\n","            gt = index[0]\n","            prd = index[1]\n","            prd_max_prob = index[2]\n","#        for gt, prd, prd_max_prob in zip(labels, preds_str, preds_max_prob):\n","           \n","            if \"Attn\" in opt.Prediction:\n","                prd_EOS = prd.find(\"[EOS]\")\n","                prd = prd[:prd_EOS]  # prune after \"end of sentence\" token ([EOS])\n","                prd_max_prob = prd_max_prob[:prd_EOS]\n","\n","            \"\"\"\n","            In our experiment, if the model predicts at least one [UNK] token, we count the word prediction as incorrect.\n","            To not take account of [UNK] token, use the below line.\n","            prd = prd.replace('[UNK]', '') \n","            \"\"\"\n","\n","            # To evaluate 'case sensitive model' with alphanumeric and case insensitve setting. = same with ASTER\n","            gt = gt.lower()\n","            prd = prd.lower()\n","            alphanumeric_case_insensitve = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n","            out_of_alphanumeric_case_insensitve = f\"[^{alphanumeric_case_insensitve}]\"\n","            gt = re.sub(out_of_alphanumeric_case_insensitve, \"\", gt)\n","            prd = re.sub(out_of_alphanumeric_case_insensitve, \"\", prd)\n","\n","            if opt.NED:\n","                # ICDAR2019 Normalized Edit Distance\n","                if len(gt) == 0 or len(prd) == 0:\n","                    norm_ED += 0\n","                elif len(gt) > len(prd):\n","                    norm_ED += 1 - edit_distance(prd, gt) / (len(gt)*0.9)\n","                else:\n","                    norm_ED += 1 - edit_distance(prd, gt) / (len(prd)*0.9)\n","\n","            else:\n","                if prd == gt:\n","                    n_correct += 1\n","\n","            # calculate confidence score (= multiply of prd_max_prob)\n","            try:\n","                confidence_score = prd_max_prob.cumprod(dim=0)[-1]\n","            except:\n","                confidence_score = 0  # for empty pred case, when prune after \"end of sentence\" token ([EOS])\n","            confidence_score_list.append(confidence_score)\n","\n","    if opt.NED:\n","        # ICDAR2019 Normalized Edit Distance. In web page, they report % of norm_ED (= norm_ED * 100).\n","        score = norm_ED / float(length_of_data*0.9) * 100\n","    else:\n","        score = n_correct / float(length_of_data*0.9) * 100  # accuracy\n","\n","    return (\n","        valid_loss_avg.val(),\n","        score,\n","        preds_str,\n","        confidence_score_list,\n","        labels,\n","        infer_time,\n","        length_of_data,\n","    )\n","\n","\n","def test(opt):\n","    converter_CTC = CTCLabelConverter(opt.character)\n","    converter = AttnLabelConverter(opt.character)\n","    opt.sos_token_index = converter.dict[\"[SOS]\"]\n","    opt.eos_token_index = converter.dict[\"[EOS]\"]\n","    opt.num_class_CTC = len(converter_CTC.character)\n","    opt.num_class = len(converter.character)\n","\n","    model = Model(opt)\n","    print(\n","        \"model input parameters\",\n","        opt.imgH,\n","        opt.imgW,\n","        opt.num_fiducial,\n","        opt.input_channel,\n","        opt.output_channel,\n","        opt.hidden_size,\n","        opt.num_class,\n","        opt.batch_max_length,\n","        opt.Transformation,\n","        opt.FeatureExtraction,\n","        opt.SequenceModeling,\n","        opt.Prediction,\n","    )\n","    model = torch.nn.DataParallel(model).to(device)\n","\n","    # load model\n","    print(\"loading pretrained model from %s\" % opt.saved_model)\n","    try:\n","        model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n","    except:\n","        print(\n","            \"*** pretrained model not match strictly *** and thus load_state_dict with strict=False mode\"\n","        )\n","        # pretrained_state_dict = torch.load(opt.saved_model)\n","        # for name in pretrained_state_dict:\n","        #     print(name)\n","        model.load_state_dict(\n","            torch.load(opt.saved_model, map_location=device), strict=False\n","        )\n","\n","    opt.exp_name = \"_\".join(opt.saved_model.split(\"/\")[1:])\n","    # print(model)\n","\n","    \"\"\" keep evaluation model and result logs \"\"\"\n","    os.makedirs(f\"./result/{opt.exp_name}\", exist_ok=True)\n","    # os.system(f'cp {opt.saved_model} ./result/{opt.exp_name}/')\n","\n","    \"\"\" setup loss \"\"\"\n","    if \"CTC\" in opt.Prediction:\n","        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n","    else:\n","        # ignore [PAD] token\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=converter.dict[\"[PAD]\"]).to(\n","            device\n","        )\n","\n","    \"\"\" evaluation \"\"\"\n","    model.eval()\n","    with torch.no_grad():\n","        if (\n","            opt.eval_type\n","        ):  # evaluate 6 benchmark evaluation datasets or 7 additionally collected evaluation datasets\n","            benchmark_all_eval(model, criterion, converter, opt)\n","        else:\n","            log = open(f\"./result/{opt.exp_name}/log_evaluation.txt\", \"a\")\n","            AlignCollate_eval = AlignCollate(opt, mode=\"test\")\n","            eval_data, eval_data_log = hierarchical_dataset(\n","                root=opt.eval_data, opt=opt, mode=\"test\"\n","            )\n","            eval_loader = torch.utils.data.DataLoader(\n","                eval_data,\n","                batch_size=opt.batch_size,\n","                shuffle=False,\n","                num_workers=int(opt.workers),\n","                collate_fn=AlignCollate_eval,\n","                pin_memory=True,\n","            )\n","            _, score_by_best_model, _, _, _, _, _ = validation(\n","                model, criterion, eval_loader, converter, opt\n","            )\n","            log.write(eval_data_log)\n","            print(f\"{score_by_best_model:0.2f}\")\n","            log.write(f\"{score_by_best_model:0.2f}\\n\")\n","            log.close()\n","\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--eval_data\", help=\"path to evaluation dataset\")\n","    parser.add_argument(\n","        \"--eval_type\",\n","        type=str,\n","        help=\"evaluate 6 benchmark evaluation datasets or 7 additionally collected evaluation datasets |benchmark|addition|\",\n","    )\n","    parser.add_argument(\n","        \"--workers\", type=int, help=\"number of data loading workers\", default=4\n","    )\n","    parser.add_argument(\"--batch_size\", type=int, default=512, help=\"input batch size\")\n","    parser.add_argument(\n","        \"--saved_model\", required=True, help=\"path to saved_model to evaluation\"\n","    )\n","    parser.add_argument(\n","        \"--log_multiple_test\", action=\"store_true\", help=\"log_multiple_test\"\n","    )\n","    \"\"\" Data processing \"\"\"\n","    parser.add_argument(\n","        \"--batch_max_length\", type=int, default=25, help=\"maximum-label-length\"\n","    )\n","    parser.add_argument(\n","        \"--imgH\", type=int, default=32, help=\"the height of the input image\"\n","    )\n","    parser.add_argument(\n","        \"--imgW\", type=int, default=100, help=\"the width of the input image\"\n","    )\n","    parser.add_argument(\n","        \"--character\",\n","        type=str,\n","        default=\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\",\n","        help=\"character label\",\n","    )\n","    parser.add_argument(\n","        \"--NED\", action=\"store_true\", help=\"For Normalized edit_distance\"\n","    )\n","    parser.add_argument(\n","        \"--Aug\",\n","        type=str,\n","        default=\"None\",\n","        help=\"whether to use augmentation |None|Blur|Crop|Rot|\",\n","    )\n","    parser.add_argument(\n","        \"--semi\",\n","        type=str,\n","        default=\"None\",\n","        help=\"whether to use semi-supervised learning |None|PL|MT|\",\n","    )\n","    \"\"\" Model Architecture \"\"\"\n","    parser.add_argument(\"--model_name\", type=str, required=True, help=\"CRNN|TRBA\")\n","    parser.add_argument(\n","        \"--num_fiducial\",\n","        type=int,\n","        default=20,\n","        help=\"number of fiducial points of TPS-STN\",\n","    )\n","    parser.add_argument(\n","        \"--input_channel\",\n","        type=int,\n","        default=3,\n","        help=\"the number of input channel of Feature extractor\",\n","    )\n","    parser.add_argument(\n","        \"--output_channel\",\n","        type=int,\n","        default=512,\n","        help=\"the number of output channel of Feature extractor\",\n","    )\n","    parser.add_argument(\n","        \"--hidden_size\", type=int, default=256, help=\"the size of the LSTM hidden state\"\n","    )\n","\n","    opt = parser.parse_args()\n","\n","    if opt.model_name == \"CRNN\":\n","        opt.Transformation = \"None\"\n","        opt.FeatureExtraction = \"VGG\"\n","        opt.SequenceModeling = \"BiLSTM\"\n","        opt.Prediction = \"CTC\"\n","\n","    elif opt.model_name == \"TRBA\":\n","        opt.Transformation = \"TPS\"\n","        opt.FeatureExtraction = \"ResNet\"\n","        opt.SequenceModeling = \"BiLSTM\"\n","        opt.Prediction = \"Attn\"\n","\n","    elif opt.model_name == \"RBA\":  # RBA\n","        opt.Transformation = \"None\"\n","        opt.FeatureExtraction = \"ResNet\"\n","        opt.SequenceModeling = \"BiLSTM\"\n","        opt.Prediction = \"Attn\"\n","\n","    cudnn.benchmark = True\n","    cudnn.deterministic = True\n","    opt.num_gpu = torch.cuda.device_count()\n","    if opt.num_gpu > 1:\n","        print(\n","            \"We recommend to use 1 GPU, check your GPU number, you would miss CUDA_VISIBLE_DEVICES=0 or typo\"\n","        )\n","        print(\"To use multi-gpu setting, remove or comment out these lines\")\n","        sys.exit()\n","\n","    if sys.platform == \"win32\":\n","        opt.workers = 0\n","\n","    os.makedirs(f\"./evaluation_log\", exist_ok=True)\n","\n","    \n","    opt.Transformation_CTC = \"None\"\n","    opt.FeatureExtraction_CTC = \"VGG\"\n","    opt.SequenceModeling_CTC = \"BiLSTM\"\n","    opt.Prediction_CTC = \"CTC\"\n","    opt.saved_model_CTC = \"/content/STR-Fewer-Labels/CRNN-PR.pth\"\n","    opt.eval_type == \"addition\"\n","\n","    test(opt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"iX6DlU2jJB3r","executionInfo":{"status":"ok","timestamp":1647530589814,"user_tz":-480,"elapsed":422,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"0344980d-d6db-4c38-ad5c-1d586f509aff"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting test.py\n"]}]},{"cell_type":"code","source":["%%time \n","!python3 demo.py --model_name TryNN --image_folder demo_image/ --saved_model /content/STR-Fewer-Labels/TRBA-PR.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCMGsO1RnxJt","executionInfo":{"status":"ok","timestamp":1647526243493,"user_tz":-480,"elapsed":194122,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"a5118765-1c63-48cd-ab72-9e76e43f3842"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["# of tokens and characters: 99\n","model input parameters 32 100 20 3 512 256 99 25 TPS ResNet BiLSTM Attn\n","loading pretrained model from /content/STR-Fewer-Labels/TRBA-PR.pth\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","# of tokens and characters: 98\n","No Transformation module specified\n","model input parameters 32 100 20 3 512 256 98 25 None VGG BiLSTM CTC\n","loading pretrained model from /content/STR-Fewer-Labels/CRNN-PR.pth\n","--------------------------------------------------------------------------------\n","image_path               \tpredicted_labels         \tconfidence score\n","--------------------------------------------------------------------------------\n","demo_image/1.png         \tCoca-Cola                \t0.9006\n","demo_image/2.png         \thire                     \t0.3715\n","demo_image/3.png         \tLaugh                    \t0.2180\n","demo_image/4.png         \tCafe                     \t0.9939\n","demo_image/5.jpg         \tFARREOD                  \t0.2645\n","demo_image/6.jpg         \tPEOPLE                   \t0.9118\n","demo_image/7.png         \tEsciting                 \t0.6025\n","demo_image/8.png         \tSigns                    \t0.9959\n","demo_image/9.jpg         \tBALLYS                   \t0.8640\n","demo_image/10.jpg        \tSHAKESHACK               \t0.2939\n","CPU times: user 1.04 s, sys: 142 ms, total: 1.18 s\n","Wall time: 3min 13s\n"]}]},{"cell_type":"code","source":["%%time\n","!python3 test.py --eval_type benchmark --model_name TRBA --saved_model /content/STR-Fewer-Labels/TRBA-PR.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RZGxc6jnyf-","executionInfo":{"status":"ok","timestamp":1647530442718,"user_tz":-480,"elapsed":27940,"user":{"displayName":"Joke Lim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11392240441572340728"}},"outputId":"b25ceb94-9ae8-4816-99a3-1739ddd7a088"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["# of tokens and characters: 98\n","# of tokens and characters: 99\n","model input parameters 32 100 20 3 512 256 99 25 TPS ResNet BiLSTM Attn\n","loading pretrained model from /content/STR-Fewer-Labels/TRBA-PR.pth\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/IIIT5k_3000\t dataset: /\n","sub-directory:\t/.\t num samples: 3000\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Acc 95.15\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/SVT\t dataset: /\n","sub-directory:\t/.\t num samples: 647\n","Acc 92.05\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/IC13_1015\t dataset: /\n","sub-directory:\t/.\t num samples: 1015\n","Acc 94.58\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/IC15_2077\t dataset: /\n","sub-directory:\t/.\t num samples: 2077\n","Acc 80.94\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/SVTP\t dataset: /\n","sub-directory:\t/.\t num samples: 645\n","Acc 81.31\n","--------------------------------------------------------------------------------\n","dataset_root:    data_CVPR2021/evaluation/benchmark/CUTE80\t dataset: /\n","sub-directory:\t/.\t num samples: 288\n","Acc 91.05\n","--------------------------------------------------------------------------------\n","accuracy: IIIT5k_3000: 95.15\tSVT: 92.05\tIC13_1015: 94.58\tIC15_2077: 80.94\tSVTP: 81.31\tCUTE80: 91.05\ttotal_accuracy: 89.65\taveraged_infer_time: 1.629\t# parameters: 49.82\n","95.15\t92.05\t94.58\t80.94\t81.31\t91.05\n","Total_accuracy:89.65\n","CPU times: user 210 ms, sys: 38.5 ms, total: 249 ms\n","Wall time: 27.8 s\n"]}]}]}